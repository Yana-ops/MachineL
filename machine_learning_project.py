# -*- coding: utf-8 -*-
"""Machine Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JVmSMnJOkL8ZJKcorTdVsCe7iaiSbMX_

# 1. Data Exploration
* Count class/label distributions (if any target column exists)
* Visualizations (distributions, scatter plots, correlation heatmaps)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
!git clone https://github.com/Yana-ops/MachineL.git
# %cd /content/MachineL

df = pd.read_csv('final_project_dataset_complete.csv')

# Display the first few rows
print(df.head())

print(df.shape)

# Get an overview of the dataset
print(df.info())

# Check the basic statistics for numerical features
print(df.describe())
# Check for missing values
print(df.isnull().sum())
# List all columns and their data types
print(df.dtypes)

# Bar plot for a categorical column (e.g., 'category_1')
df['category_1'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Distribution of category_1')
plt.xlabel('Category_1')
plt.ylabel('Frequency')
plt.show()

# Bar plot for a categorical column (e.g., 'category_2')
df['category_2'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Distribution of Regions')
plt.xlabel('Regions')
plt.ylabel('Frequency')
plt.show()

df.hist(bins=30, figsize=(15,10), edgecolor='black')
plt.suptitle('Distribution of Numeric Features', fontsize=16)
plt.show()

plt.figure(figsize=(6,6))
df["category_1"].value_counts().plot.pie(autopct='%1.1f%%')
plt.title("Category 1 Distribution")
plt.show()

plt.figure(figsize=(6,6))
df["category_2"].value_counts().plot.pie(autopct='%1.1f%%')
plt.title("Category 2 Distribution")
plt.show()

# Select only numeric columns
numeric_df = df.select_dtypes(include=['number'])
print(numeric_df)

# Compute the correlation matrix
corr_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

# Define features for the scatter plot
x = df["feature_1"]
y = df["feature_2"]

# Scatter plot
plt.plot(x, y, "b.", alpha=0.6)  # Blue dots with some transparency
plt.xlabel("$Feature_1$", fontsize=8)  # x-axis label
plt.ylabel("$Feature_2$", rotation=90, fontsize=8)  # y-axis label
plt.title("Scatter Plot: Feature_1 vs Feature_2", fontsize=16)
plt.axis([x.min() - 0.5, x.max() + 0.5, y.min() - 0.5, y.max() + 0.5])  # Set axis limits
plt.grid(True)  # Enable grid for better readability
plt.show()

import itertools

# Set seaborn style for better aesthetics
sns.set(style="whitegrid")

# Select only numeric columns
numeric_columns = df.select_dtypes(include=['number']).columns
print(numeric_columns)

# Check if there are at least two numeric columns to plot
if len(numeric_columns) < 2:
    print("Not enough numeric columns to generate scatter plots.")
else:
    # Generate scatter plot combinations
    combinations = list(itertools.combinations(numeric_columns, 2))
    n_combinations = len(combinations)

    # Define grid size for subplots
    cols = 4  # Number of columns in the grid
    rows = (n_combinations + cols - 1) // cols  # Calculate required rows

    # Create a single figure
    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 5 * rows), constrained_layout=True)
    axes = axes.flatten()  # Flatten the axes for easy indexing

    for idx, (feature_x, feature_y) in enumerate(combinations):
        ax = axes[idx]  # Get the current subplot

        # Define x and y for the current pair
        x = df[feature_x]
        y = df[feature_y]

        # Scatter plot
        ax.plot(x, y, "b.", alpha=0.6)  # Blue dots with transparency
        ax.set_xlabel(feature_x, fontsize=10, labelpad=5)  # x-axis label
        ax.set_ylabel(feature_y, fontsize=10, labelpad=5)  # y-axis label
        #ax.tick_params(axis='x', labelrotation=45)
        ax.set_title(f"{feature_x} vs {feature_y}", fontsize=12)

        # Dynamically adjust axis limits with buffer
        x_buffer = (x.max() - x.min()) * 0.05
        y_buffer = (y.max() - y.min()) * 0.05
        ax.set_xlim(x.min() - x_buffer, x.max() + x_buffer)
        ax.set_ylim(y.min() - y_buffer, y.max() + y_buffer)

        ax.grid(True)  # Enable grid for better readability

    # Hide unused subplots if any
    for idx in range(len(combinations), len(axes)):
        fig.delaxes(axes[idx])

    # Adjust layout to prevent overlapping
    fig.tight_layout()
    fig.subplots_adjust(hspace=0.4, wspace=0.3)  # Adjust horizontal and vertical spacing
    plt.show()

"""# 2. Data Preprocessing


*   Activity: Data Cleaning and Preparation

*   Description: Handle missing values, detect and treat outliers, and encode categorical variables.

*   Deliverables: D2.1) Section 2 of the final report (titled “Data Preprocessing“) detailing the preprocessing steps; D2.2) A cleaned dataset.

*   Guidelines:


*   Choose appropriate methods to handle missing values (e.g., imputation, removal).

*   Identify and treat outliers using techniques such as clipping or transformation.


*   Encode categorical variables using one-hot encoding or label encoding.

*   Normalize or scale numerical features if necessary
"""

missing_count = df.isnull().sum() # Count of missing values
missing_percent = (missing_count / len(df)) * 100 # Percentage of missing values

# Combine into one DataFrame
missing_summary = pd.DataFrame({
    'Missing Values': missing_count,
    'Missing Percentage (%)': missing_percent.round(2)
})

missing_summary = missing_summary[missing_summary['Missing Values'] > 0] # Filter only columns with missing values

print("Missing Value Summary:")
print(missing_summary)

df.drop(columns='feature_2', inplace=True)
#Since it is redundant as we could obsever from the correlation matrix

print(df.dtypes)

"""**Missing Values -> Mean Imputation**

"""

from sklearn.impute import SimpleImputer

# Imputation for numerical columns
num_imp = SimpleImputer(strategy='mean')
df['feature_3'] = num_imp.fit_transform(df[['feature_3']])

df['feature_6'] = num_imp.fit_transform(df[['feature_6']])

print("Missing values:\n", df.isnull().sum())

"""**Identify and Treat Outliers -> IQR Clipping**"""

# IQR-based Clipping to Treat Outliers
numerical_columns = ['feature_1', 'feature_3', 'feature_4',
                     'feature_5', 'feature_6', 'feature_7', 'feature_8']

for col in numerical_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df[col] = np.clip(df[col], lower, upper)

"""**Encoding ->	One-Hot & Label Enc.**"""

# Label encoding for 'category_1'
mapping = {"Low": 0, 'Below Average': 1, 'Above Average': 2, 'High': 3}
df['category_1_encoded'] = df['category_1'].map(mapping)

df.drop(columns=['category_1'], inplace=True)

# One-hot encoding for 'category_2'
df = pd.get_dummies(df, columns=['category_2'])
bool_columns = df.select_dtypes(include='bool').columns
df[bool_columns] = df[bool_columns].astype(int)

# Display first 10 rows for category_1_encoded and category_2 one-hot columns
df[['category_1_encoded', 'category_2_Region A', 'category_2_Region B', 'category_2_Region C']].head(5)

print("Unique values in encoded columns:\n", df[['category_1_encoded']].nunique())

"""**Scaling -> StandardScaler**"""

from sklearn.preprocessing import StandardScaler

numerical_columns = ['feature_1','feature_3', 'feature_4',
                     'feature_5', 'feature_6', 'feature_7', 'feature_8']
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

df.hist(bins=30, figsize=(15,10), edgecolor='black')
plt.suptitle('Distribution of Features', fontsize=16)
plt.show()

# Save cleaned dataset to CSV
df.to_csv('cleaned_final_dataset.csv', index=False)

#To download:
# from google.colab import files
# files.download('cleaned_final_dataset.csv')

"""# 3. Exploratory Data Analysis (EDA)  


* Activity: In-depth EDA  
*   Description: Perform detailed EDA to understand key factors.  
*   Deliverables: D3.1) Section 3 of the final report (titled “Exploratory Data Analysis”), a comprehensive EDA report with visualizations and insights.
*   Guidelines:  
*   Investigate the relationship between features and the target variable.  
*   Use visualization techniques like bar charts, box plots, and heatmaps.  
*   Perform hypothesis testing where applicable (e.g., t-tests, chi-square tests).
"""

print(df.head()) #Display the first few rows

print(df.describe()) #Show descriptive statistics

"""Mean values grouped by target"""

df.groupby('target').mean(numeric_only=True)

"""Bar chart for categorical feature"""

# Identify all categorical columns to test (encoded + dummies)
categorical_cols = ['category_1_encoded'] + [col for col in df.columns if col.startswith('category_2_')]

# Create count plots for each categorical feature vs target
for col in categorical_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(x=col, hue='target', data=df, palette='Set2')
    plt.title(f'Count Plot: {col} vs Target')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.tight_layout()
    plt.show()

"""Box Plots for Numerical Features"""

# Box plots for only feature_1 to feature_8
numeric_features = [f for f in [f'feature_{i}' for i in range(1, 9)] if f in df.columns]

for feature in numeric_features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x='target', y=feature, data=df)
    plt.title(f'Boxplot of {feature} by Target')
    plt.xlabel('Target')
    plt.ylabel(feature)
    plt.tight_layout()
    plt.show()

# Another example for comparing 'feature_1' with 'category_1'
plt.figure(figsize=(8, 6))
sns.boxplot(x='category_1_encoded', y='feature_1', data=df, palette='coolwarm',  hue='category_1_encoded', legend=False)
plt.title('Boxplot of Feature 1 by Category 1')
plt.xlabel('Category 1')
plt.ylabel('Feature 1')
plt.show()
plt.tight_layout()

"""Category_1_encoded represents: 0 = Low, 1 = Below Average, 2 = Above Average,3 = High

feature_1 might be something like like "spending score". A boxplot showing increasing medians across categories might suggest that people rated as 'High' have higher feature_1 scores — potentially useful for modeling.

T-test
"""

from scipy.stats import ttest_ind, chi2_contingency

# Check unique values in target
print(df['target'].unique())

# Replace 'ActualValue1' and 'ActualValue2' with real categories from target
target_values = df['target'].unique()
group1 = df[df['target'] == target_values[0]]['feature_1']
group2 = df[df['target'] == target_values[1]]['feature_1']

# Perform T-test
target_values = df['target'].unique()
group1 = df[df['target'] == target_values[0]]
group2 = df[df['target'] == target_values[1]]

numeric_features = df.select_dtypes(include='number').columns.drop('target')

print("T-test results:")
for feature in numeric_features:
    t_stat, p_val = ttest_ind(group1[feature], group2[feature], nan_policy='omit')
    print(f"{feature}: t = {t_stat:.3f}, p = {p_val:.5f}")

"""Chi-square test"""

# Chi-square test for categorical features
from scipy.stats import chi2_contingency

categorical_columns = ['category_1_encoded'] + [col for col in df.columns if col.startswith('category_2_')]

print("Chi-square test results:")
for col in categorical_columns:
    contingency_table = pd.crosstab(df[col], df['target'])
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    print(f"{col}: chi2 = {chi2_stat:.3f}, p = {p_val:.5f}")

"""Heatmaps for Correlations"""

# Compute correlation matrix
corr_matrix = df.select_dtypes(include=['number']).corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""# 4. Feature Engineering

* Activity: Create New Features

* Description: Engineer new features that could improve model performance.

* Deliverables: D4.1) Section 4 of the final report (title "Feature Engineering"), explaining their creation and potential impact; D4.2) A dataset with new features.

Guidelines:

* Combine existing features to create new ones (e.g., interaction terms, ratios).

* Use domain knowledge to add relevant features (e.g., total service usage).

* Justify the inclusion of each new feature with potential benefits.
"""

# 1.Sum of Most Correlated Features
df['feature_1_5_mean'] = df[['feature_1', 'feature_3', 'feature_4', 'feature_5']].mean(axis=1)

# Output visualization
print("feature_1_5_mean:")
print(df['feature_1_5_mean'].head())

# 2. Ratio Feature
df['feature_1_3_ratio'] = df['feature_1'] / (df['feature_3'] + 1e-5)

# Output visualization
print("feature_1_3_ratio")
print(df['feature_1_3_ratio'].head())

# 3. Iteraction Term
df['feature_1_4_interaction'] = df['feature_1'] * df['feature_4']

# Output visualization
print("feature_1_4_interaction:")
print(df[['feature_1_4_interaction']].head())

# 4. Compute total service usage as the sum of all numerical features
df["total_service_usage"] = df["feature_1"] + df["feature_3"] + df["feature_4"] + df["feature_5"] + df["feature_6"] + df["feature_7"] + df["feature_8"]

# Verify the new feature
print("After adding 'total_service_usage':")
print(df[["total_service_usage"]].head())

print(df["total_service_usage"].describe())

print(df.columns)

# List of your engineered features
engineered_features = [
    'feature_1_5_mean',
    'feature_1_3_ratio',
    'feature_1_4_interaction',
    'total_service_usage'
]

# Check for missing values

missing_values = df[engineered_features].isnull().sum()

# Combine both into a readable summary
missing_summary = pd.DataFrame({
    'Missing Count': missing_values,
})

print("Missing Values in Engineered Features:\n")
print(missing_summary)

engineered = ['feature_1_5_mean', 'feature_1_3_ratio', 'feature_1_4_interaction', 'total_service_usage']
df[engineered] = scaler.fit_transform(df[engineered])

# Save cleaned dataset to CSV
df.to_csv('newFeatures_cleaned_final_dataset.csv', index=False)

# To download:
from google.colab import files
files.download('newFeatures_cleaned_final_dataset.csv')

"""# 5. Modeling Activity: Build and Evaluate Models  

*   Description: Train different machine learning models. No less than 3 ensemble learning models must be trained.  
*   Deliverables: D5.1) Section 5 of the final report comparing modelperformances;
D5.2) Trained models.
*   Guidelines:  
*   Split the data into training and testing sets.  
*   Train several models (e.g., logistic regression, decision trees, random forest, gradient boosting, ensemble learning models).  
*   Evaluate models using metrics like accuracy, precision, recall, F1-score, and ROC-AUC.  
*   Perform cross-validation to ensure robust performance estimates.
"""

# Re-define features and target (only if X and y are not available)
X = df.drop(columns=['target'])
y = df['target']

# Split data into train and test sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Import models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Support Vector Machine': SVC(kernel='rbf', probability=True)
}

# Evaluate models
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)

results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate probability scores for ROC-AUC
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)[:, 1]
    else:
        y_prob = model.decision_function(X_test)

    results[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_prob),
        'Confusion Matrix': confusion_matrix(y_test, y_pred),
        'Classification Report': classification_report(y_test, y_pred)
    }

# Create evaluation summary
evaluation_df = pd.DataFrame({
    model: {
        'Accuracy': metrics['Accuracy'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall'],
        'F1-score': metrics['F1-score'],
        'ROC-AUC': metrics['ROC-AUC']
    } for model, metrics in results.items()
}).T

print("Model Evaluation Summary:")
print(evaluation_df)

from sklearn.metrics import roc_curve, auc

# ROC Curves - With Features
# print("\n ROC Curves (With Engineered Features)")
# for name, model in models.items():
#     if hasattr(model, "predict_proba"):
#         y_prob = model.predict_proba(X_test)[:, 1]
#     else:
#         y_prob = model.decision_function(X_test)

#     fpr, tpr, _ = roc_curve(y_test, y_prob)
#     roc_auc = auc(fpr, tpr)
#     plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

# plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
# plt.title('ROC Curve (with engineered features)')
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.legend()
# plt.grid(True)
# plt.show()

# Plot confusion matrices
# for name, metrics in results.items():
#     plt.figure(figsize=(5, 4))
#     sns.heatmap(metrics['Confusion Matrix'], annot=True, fmt="d", cmap="Blues")
#     plt.title(f'Confusion Matrix: {name}')
#     plt.xlabel('Predicted')
#     plt.ylabel('Actual')
#     plt.tight_layout()
#     plt.show()

# Print classification reports
for name, metrics in results.items():
    print(f"\n{name} - Classification Report:\n")
    print(metrics['Classification Report'])

# Drop engineered features from X
X_baseline = X.drop(columns=engineered_features, errors='ignore')

# Split into training and testing
Xb_train, Xb_test, yb_train, yb_test = train_test_split(X_baseline, y, test_size=0.2, random_state=42)

# Train and evaluate
baseline_results = {}

for name, model in models.items():
    model.fit(Xb_train, yb_train)
    yb_pred = model.predict(Xb_test)

    # For ROC-AUC
    if hasattr(model, "predict_proba"):
        yb_proba = model.predict_proba(Xb_test)[:, 1]
    else:
        yb_proba = model.decision_function(Xb_test)

    # Store results
    baseline_results[name] = {
        'Accuracy': accuracy_score(yb_test, yb_pred),
        'Precision': precision_score(yb_test, yb_pred),
        'Recall': recall_score(yb_test, yb_pred),
        'F1-score': f1_score(yb_test, yb_pred),
        'ROC-AUC': roc_auc_score(yb_test, yb_proba),
        'Classification Report': classification_report(yb_test, yb_pred)
    }

# Step 5: Print detailed metrics for each model
for name, metrics in baseline_results.items():
    print(f"\n{name} - Performance WITHOUT Engineered Features:")
    print(f"Accuracy: {metrics['Accuracy']:.4f}")
    print(f"Precision: {metrics['Precision']:.4f}")
    print(f"Recall: {metrics['Recall']:.4f}")
    print(f"F1-score: {metrics['F1-score']:.4f}")
    print(f"ROC-AUC: {metrics['ROC-AUC']:.4f}")
    print(metrics['Classification Report'])

# Step 6: Create and print summary table
baseline_summary_df = pd.DataFrame({
    model: {
        'Accuracy': metrics['Accuracy'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall'],
        'F1-score': metrics['F1-score'],
        'ROC-AUC': metrics['ROC-AUC']
    }
    for model, metrics in baseline_results.items()
}).T

print("\nModel Evaluation Summary (Without Engineered Features):")
print(baseline_summary_df.round(4))

# ROC Curves - Without Features
# print("\n ROC Curves (Without Engineered Features)")
# for name, model in models.items():
#     if hasattr(model, "predict_proba"):
#         yb_prob = model.predict_proba(Xb_test)[:, 1]
#     else:
#         yb_prob = model.decision_function(Xb_test)

#     fpr, tpr, _ = roc_curve(yb_test, yb_prob)
#     roc_auc = auc(fpr, tpr)

#     plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

# plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
# plt.title('ROC Curve (without engineered features)')
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.legend()
# plt.grid(True)
# plt.show()

# # Plot confusion matrices for models without features
# print("\n Confusion Matrices (Without Engineered Features)")
# for name, metrics in baseline_results.items():
#     cm = metrics['Confusion Matrix'] if 'Confusion Matrix' in metrics else confusion_matrix(yb_test, model.predict(Xb_test))
#     plt.figure(figsize=(5, 4))
#     sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges')
#     plt.title(f'Confusion Matrix: {name} (no features)')
#     plt.xlabel('Predicted')
#     plt.ylabel('Actual')
#     plt.tight_layout()
#     plt.show()

# CROSS-VALIDATION — Already Imputed Data
from sklearn.model_selection import cross_validate

# Step 1: Define scoring metrics
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

# Step 2: Perform 5-fold cross-validation using cleaned and imputed data
cv_results = {}

for name, model in models.items():
    scores = cross_validate(
        model, X, y,  # X and y already preprocessed and imputed
        scoring=scoring,
        cv=5,
        return_train_score=False
    )
    cv_results[name] = {
        'Accuracy (CV avg)': scores['test_accuracy'].mean(),
        'Precision (CV avg)': scores['test_precision'].mean(),
        'Recall (CV avg)': scores['test_recall'].mean(),
        'F1-score (CV avg)': scores['test_f1'].mean(),
        'ROC-AUC (CV avg)': scores['test_roc_auc'].mean()
    }

# Step 3: Display cross-validation results
cv_summary = pd.DataFrame(cv_results).T
print("\nCross-Validation Metrics Summary (5-Fold):")
print(cv_summary.round(4))

# Combine and Compare Evaluation Summaries Side by Side
# Rename rows for clarity
evaluation_df.index = [f"{model} (with features)" for model in evaluation_df.index]
baseline_summary_df.index = [f"{model} (no features)" for model in baseline_summary_df.index]

# Concatenate both summaries
combined_summary = pd.concat([evaluation_df, baseline_summary_df])

# Optional: sort by model type for grouped view
combined_summary = combined_summary.sort_index()

# Print or display the combined table
print("Combined Model Evaluation Summary:")
print(combined_summary.round(4))

import pickle
from google.colab import files

# Loop over each model and save to a unique .pkl file
for name, model in models.items():
    filename = f"{name.replace(' ', '_').lower()}_model.pkl"  # e.g., logistic_regression_model.pkl
    with open(filename, 'wb') as f:
        pickle.dump(model, f)
    print(f"Saved: {filename}")
    files.download(filename)

"""# 6. Model Tuning Activity: Hyperparameter Tuning

*   Description: Optimize model hyperparameters to improve performance.  
*   Deliverables: D6.1) Section 6 of the final report (titled “Hyperparameter tuning”) on the tuning process and results;  
D6.2) Best-tuned models.  

Guidelines:  

*   Use techniques like grid search or randomized search for hyperparameter tuning.  
*   Compare the performance of tuned models against default models.  
*   Document the tuning process and the chosen hyperparameters.
"""

# HYPERPARAMETER TUNING (Random Forest)
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# Step 1: Define parameter distributions and grids
param_dist = {
    'n_estimators': [15, 25, 50],
    'max_depth': [10, 12],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [2, 3]
}

# Step 2: RandomizedSearchCV
rf_random = RandomForestClassifier(random_state=42)
random_search = RandomizedSearchCV(
    rf_random,
    param_distributions=param_dist,
    scoring='accuracy',
    cv=3,
    n_iter=10,
    random_state=42
)
random_search.fit(X, y)

print("RandomizedSearchCV for Random Forest")
print("Best Parameters:", random_search.best_params_)
print("Best Accuracy Score:", round(random_search.best_score_, 4))

# Step 3: GridSearchCV (exhaustive search)
param_grid = {
    'n_estimators': [15, 25, 50],
    'max_depth': [10, 12],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [2, 3]
}

rf_grid = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    rf_grid,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3
)
grid_search.fit(X, y)

print("\nGridSearchCV for Random Forest")
print("Best Parameters:", grid_search.best_params_)
print("Best Accuracy Score:", round(grid_search.best_score_, 4))

from sklearn.linear_model import LogisticRegression

# Step 1: Define parameter distributions and grids
param_dist_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear', 'lbfgs', 'saga']
}

# Step 2: RandomizedSearchCV
lr = LogisticRegression(max_iter=1000, random_state=42)
random_search_lr = RandomizedSearchCV(
    lr,
    param_distributions=param_dist_lr,
    scoring='accuracy',
    cv=3,
    n_iter=10,
    random_state=42
)
random_search_lr.fit(X, y)

print("\nRandomizedSearchCV for Logistic Regression")
print("Best Parameters:", random_search_lr.best_params_)
print("Best Accuracy Score:", round(random_search_lr.best_score_, 4))

# Step 3: GridSearchCV
param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['liblinear', 'lbfgs']
}

grid_search_lr = GridSearchCV(
    LogisticRegression(max_iter=1000, random_state=42),
    param_grid=param_grid_lr,
    scoring='accuracy',
    cv=3
)
grid_search_lr.fit(X, y)

print("\nGridSearchCV for Logistic Regression")
print("Best Parameters:", grid_search_lr.best_params_)
print("Best Accuracy Score:", round(grid_search_lr.best_score_, 4))

from scipy.stats import loguniform

# Step 1: Define faster, focused parameter distributions for RandomizedSearchCV
param_dist_svm = {
    'C': loguniform(0.1, 10),  # Sample C from a continuous range
    'gamma': ['scale', 'auto'],
    'kernel': ['rbf']  # Use only 'rbf' kernel for speed
}

# Step 2: RandomizedSearchCV
svm = SVC(probability=False, random_state=42)  # Disable probability=True for speed
random_search_svm = RandomizedSearchCV(
    estimator=svm,
    param_distributions=param_dist_svm,
    scoring='accuracy',
    cv=2,
    n_iter=5,            # Fewer iterations for speed
    random_state=42,
    n_jobs=-1            # Use all CPU cores
)
random_search_svm.fit(X, y)

print("\nRandomizedSearchCV for SVM")
print("Best Parameters:", random_search_svm.best_params_)
print("Best Accuracy Score:", round(random_search_svm.best_score_, 4))

# Step 3: Define smaller grid for GridSearchCV
param_grid_svm = {
    'C': [1, 10],
    'gamma': ['scale'],
    'kernel': ['rbf']
}

# Step 4: GridSearchCV
grid_search_svm = GridSearchCV(
    estimator=SVC(probability=False, random_state=42),
    param_grid=param_grid_svm,
    scoring='accuracy',
    cv=2,
    n_jobs=-1
)
grid_search_svm.fit(X, y)

print("\nGridSearchCV for SVM")
print("Best Parameters:", grid_search_svm.best_params_)
print("Best Accuracy Score:", round(grid_search_svm.best_score_, 4))

import pickle
from google.colab import files

# Extract the best models from GridSearchCV (you can change to random_search_* if preferred)
best_logistic = grid_search_lr.best_estimator_
best_rf = grid_search.best_estimator_
best_svm = grid_search_svm.best_estimator_

# Store in dictionary
tuned_models = {
    'logistic_regression_best_model.pkl': best_logistic,
    'random_forest_best_model.pkl': best_rf,
    'svm_best_model.pkl': best_svm
}

# Save and download each as a separate file
for filename, model in tuned_models.items():
    with open(filename, 'wb') as f:
        pickle.dump(model, f)
    print(f"Saved: {filename}")
    files.download(filename)

"""# 7. Model Interpretation Activity: Interpret the Model

*   Description: Analyze the models to understand which features are most important for predictions.
*   Deliverables: D7.1) Section 7 of the final report (titled “Model interpretation”) on feature importance and model interpretation.  
*   Guidelines:  
*   Use feature importance scores from models like decision trees or random forests.  
*   Apply SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable
Model-agnostic Explanations) for detailed model interpretation.  
*   Discuss how the insights align with domain knowledge and business logic.
"""

pip install lime

import shap
import lime
from lime.lime_tabular import LimeTabularExplainer

# Reuse X and y from previous steps
# Train a Random Forest for interpretability
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Get and sort feature importances
importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=True)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x=importances.values[-10:], y=importances.index[-10:], hue=importances.index[-10:], palette="viridis", legend=False)
plt.title("Top 10 Most Important Features (Random Forest)")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.tight_layout()
plt.grid(False)
plt.show()

# Step 5: LIME explainer for all models (first test instance)
feature_names = X_train.columns.tolist()
class_names = y_train.unique().astype(str).tolist()

explainer = LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=feature_names,
    class_names=class_names,
    mode='classification'
)

i = 0  # Index of the test instance to explain

for name, model in models.items():
    print(f"\n=== LIME explanation for {name} on test instance {i} ===\n")
    exp = explainer.explain_instance(
        data_row=X_test.iloc[i].values,
        predict_fn=model.predict_proba if hasattr(model, "predict_proba") else model.decision_function
    )
    exp.show_in_notebook(show_table=True, show_all=False)