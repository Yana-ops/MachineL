# -*- coding: utf-8 -*-
"""Machine Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JVmSMnJOkL8ZJKcorTdVsCe7iaiSbMX_

# 1. Data Exploration
* Count class/label distributions
* Visualizations (distributions, scatter plots, correlation heatmaps)
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!git clone https://github.com/Yana-ops/MachineL.git
# %cd /content/MachineL

df = pd.read_csv('final_project_dataset_complete.csv')

pip install lime # For 7 Section (run once for downloading)

# Display the first few rows
df.head(10)

print(df.shape)

# Get an overview of the dataset
print(df.info())

# Check the basic statistics for numerical features
print(df.describe())
# Check for missing values
print(df.isnull().sum())
# List all columns and their data types
print(df.dtypes)

# Bar plot for a categorical column (e.g., 'category_1')
df['category_1'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Distribution of category_1')
plt.xlabel('Category_1')
plt.ylabel('Frequency')
plt.show()

# Bar plot for a categorical column (e.g., 'category_2')
df['category_2'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')
plt.title('Distribution of Regions')
plt.xlabel('Regions')
plt.ylabel('Frequency')
plt.show()

df.hist(bins=30, figsize=(20,15), edgecolor='black')
plt.suptitle('Distribution of Numeric Features', fontsize=16)
plt.show()

plt.figure(figsize=(6,6))
df["category_1"].value_counts().plot.pie(autopct='%1.1f%%')
plt.title("Category 1 Distribution")
plt.show()

plt.figure(figsize=(6,6))
df["category_2"].value_counts().plot.pie(autopct='%1.1f%%')
plt.title("Category 2 Distribution")
plt.show()

# Select only numeric columns
numeric_df = df.select_dtypes(include=['number'])

# Compute the correlation matrix
corr_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

# Define features for the scatter plot
x = df["feature_1"]
y = df["feature_2"]

# Scatter plot
plt.plot(x, y, "b.", alpha=0.6)
plt.xlabel("$Feature_1$", fontsize=8)
plt.ylabel("$Feature_2$", rotation=90, fontsize=8)
plt.title("Scatter Plot: Feature_1 vs Feature_2", fontsize=16)
plt.axis([x.min() - 0.5, x.max() + 0.5, y.min() - 0.5, y.max() + 0.5])  # Set axis limits
plt.grid(True)  # Enable grid for better readability
plt.show()

import itertools

# Set seaborn style for better aesthetics
sns.set(style="whitegrid")

# Select only numeric columns
numeric_columns = df.select_dtypes(include=['number']).columns

# Check if there are at least two numeric columns to plot
if len(numeric_columns) < 2:
    print("Not enough numeric columns to generate scatter plots.")
else:
    # Generate scatter plot combinations
    combinations = list(itertools.combinations(numeric_columns, 2))
    n_combinations = len(combinations)

    # Define grid size for subplots
    cols = 4
    rows = (n_combinations + cols - 1) // cols  # Calculate required rows

    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 5 * rows), constrained_layout=True)
    axes = axes.flatten()  # Flatten the axes for easy indexing

    for idx, (feature_x, feature_y) in enumerate(combinations):
        ax = axes[idx]  # Get the current subplot

        # Define x and y for the current pair
        x = df[feature_x]
        y = df[feature_y]

        # Scatter plot
        ax.plot(x, y, "b.", alpha=0.6)
        ax.set_xlabel(feature_x, fontsize=10, labelpad=5)
        ax.set_ylabel(feature_y, fontsize=10, labelpad=5)
        #ax.tick_params(axis='x', labelrotation=45)
        ax.set_title(f"{feature_x} vs {feature_y}", fontsize=12)

        # Dynamically adjust axis limits with buffer
        x_buffer = (x.max() - x.min()) * 0.05
        y_buffer = (y.max() - y.min()) * 0.05
        ax.set_xlim(x.min() - x_buffer, x.max() + x_buffer)
        ax.set_ylim(y.min() - y_buffer, y.max() + y_buffer)

        ax.grid(True)

    # Hide unused subplots if any
    for idx in range(len(combinations), len(axes)):
        fig.delaxes(axes[idx])

    # Adjust layout to prevent overlapping
    fig.tight_layout()
    fig.subplots_adjust(hspace=0.4, wspace=0.3)
    plt.show()

"""# 2. Data Preprocessing


*   Activity: Data Cleaning and Preparation

*   Description: Handle missing values, detect and treat outliers, and encode categorical variables.

*   Deliverables: D2.1) Section 2 of the final report (titled “Data Preprocessing“) detailing the preprocessing steps; D2.2) A cleaned dataset.

*   Guidelines:


*   Choose appropriate methods to handle missing values (e.g., imputation, removal).

*   Identify and treat outliers using techniques such as clipping or transformation.


*   Encode categorical variables using one-hot encoding or label encoding.

*   Normalize or scale numerical features if necessary

**Missing Values -> Mean Imputation**
"""

missing_count = df.isnull().sum()
missing_percent = (missing_count / len(df)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_count,
    'Missing Percentage (%)': missing_percent.round(2)
})

missing_summary = missing_summary[missing_summary['Missing Values'] > 0] # Filter only columns with missing values

print("Missing Value Summary:")
print(missing_summary)

from sklearn.impute import SimpleImputer

# Imputation for numerical columns
num_imp = SimpleImputer(strategy='mean')
df['feature_3'] = num_imp.fit_transform(df[['feature_3']])

df['feature_6'] = num_imp.fit_transform(df[['feature_6']])

print("Missing values:\n", df.isnull().sum())

"""**Identify and Treat Outliers -> IQR Clipping**"""

# Define numeric features to clip
numerical_columns = ['feature_1', 'feature_2', 'feature_3', 'feature_4',
                     'feature_5', 'feature_6', 'feature_7', 'feature_8']

for col in numerical_columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # Count how many values would be clipped
    original = df[col]
    n_clipped = ((original < lower) | (original > upper)).sum()

    # Apply clipping
    df[col] = np.clip(original, lower, upper)

    print(f"{col}: {n_clipped} values clipped")

"""**Encoding ->	One-Hot & Label Enc.**"""

# Label encoding for 'category_1'
mapping = {"Low": 0, 'Below Average': 1, 'Above Average': 2, 'High': 3}
df['category_1_encoded'] = df['category_1'].map(mapping)

df.drop(columns=['category_1'], inplace=True)

# One-hot encoding for 'category_2'
df = pd.get_dummies(df, columns=['category_2'])
bool_columns = df.select_dtypes(include='bool').columns
df[bool_columns] = df[bool_columns].astype(int)

# Display first 10 rows for category_1_encoded and category_2 one-hot columns
df[['category_1_encoded', 'category_2_Region A', 'category_2_Region B', 'category_2_Region C']].head(5)

print("Unique values in encoded columns:\n", df[['category_1_encoded']].nunique())

"""**Scaling -> StandardScaler**"""

from sklearn.preprocessing import StandardScaler

numerical_columns = ['feature_1','feature_2','feature_3', 'feature_4',
                     'feature_5', 'feature_6', 'feature_7', 'feature_8']
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

df.hist(bins=30, figsize=(15,15), edgecolor='black')
plt.suptitle('Distribution of Features', fontsize=16)
plt.show()

# Save cleaned dataset to CSV
df.to_csv('cleaned_final_dataset.csv', index=False)

#To download:
# from google.colab import files
# files.download('cleaned_final_dataset.csv')

"""# 3. Exploratory Data Analysis (EDA)  


* Activity: In-depth EDA  
*   Description: Perform detailed EDA to understand key factors.  
*   Deliverables: D3.1) Section 3 of the final report (titled “Exploratory Data Analysis”), a comprehensive EDA report with visualizations and insights.
*   Guidelines:  
*   Investigate the relationship between features and the target variable.  
*   Use visualization techniques like bar charts, box plots, and heatmaps.  
*   Perform hypothesis testing where applicable (e.g., t-tests, chi-square tests).
"""

print(df.head())

print(df.describe()) #Show descriptive statistics

"""Mean values grouped by target"""

df.groupby('target').mean(numeric_only=True)

"""Bar chart for categorical feature"""

# Identify all categorical columns to test (encoded + dummies)
categorical_cols = ['category_1_encoded'] + [col for col in df.columns if col.startswith('category_2_')]

# Create count plots for each categorical feature vs target
for col in categorical_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(x=col, hue='target', data=df, palette='Set2')
    plt.title(f'Count Plot: {col} vs Target')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.tight_layout()
    plt.show()

"""Box Plots for Numerical Features"""

# Box plots for only feature_1 to feature_8
numeric_features = [f for f in [f'feature_{i}' for i in range(1, 9)] if f in df.columns]

for feature in numeric_features:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x='target', y=feature, data=df)
    plt.title(f'Boxplot of {feature} by Target')
    plt.xlabel('Target')
    plt.ylabel(feature)
    plt.tight_layout()
    plt.show()

# Another example for comparing 'feature_1' with 'category_1'
plt.figure(figsize=(8, 6))
sns.boxplot(x='category_1_encoded', y='feature_1', data=df, palette='coolwarm',  hue='category_1_encoded', legend=False)
plt.title('Boxplot of Feature 1 by Category 1')
plt.xlabel('Category 1')
plt.ylabel('Feature 1')
plt.show()
plt.tight_layout()

"""Category_1_encoded represents: 0 = Low, 1 = Below Average, 2 = Above Average,3 = High

feature_1 might be something like like "spending score". A boxplot showing increasing medians across categories might suggest that people rated as 'High' have higher feature_1 scores — potentially useful for modeling.

T-test
"""

from scipy.stats import ttest_ind, chi2_contingency

# Check unique values in target
print(df['target'].unique())

# Replace 'ActualValue1' and 'ActualValue2' with real categories from target
target_values = df['target'].unique()
group1 = df[df['target'] == target_values[0]]['feature_1']
group2 = df[df['target'] == target_values[1]]['feature_1']

# Perform T-test
target_values = df['target'].unique()
group1 = df[df['target'] == target_values[0]]
group2 = df[df['target'] == target_values[1]]

numeric_features = df.select_dtypes(include='number').columns.drop('target')

print("T-test results:")
for feature in numeric_features:
    t_stat, p_val = ttest_ind(group1[feature], group2[feature], nan_policy='omit')
    print(f"{feature}: t = {t_stat:.3f}, p = {p_val:.5f}")

"""Chi-square test"""

# Chi-square test for categorical features
categorical_columns = ['category_1_encoded'] + [col for col in df.columns if col.startswith('category_2_')]

print("Chi-square test results:")
for col in categorical_columns:
    contingency_table = pd.crosstab(df[col], df['target'])
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    print(f"{col}: chi2 = {chi2_stat:.3f}, p = {p_val:.5f}")

"""Heatmaps for Correlations"""

# Compute correlation matrix
corr_matrix = df.select_dtypes(include=['number']).corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap')
plt.show()

"""# 4. Feature Engineering

* Activity: Create New Features

* Description: Engineer new features that could improve model performance.

* Deliverables: D4.1) Section 4 of the final report (title "Feature Engineering"), explaining their creation and potential impact; D4.2) A dataset with new features.

Guidelines:

* Combine existing features to create new ones (e.g., interaction terms, ratios).

* Use domain knowledge to add relevant features (e.g., total service usage).

* Justify the inclusion of each new feature with potential benefits.
"""

# 1.Sum of Most Correlated Features
df['feature_1_5_mean'] = df[['feature_1', 'feature_2','feature_3', 'feature_4', 'feature_5']].mean(axis=1)

# Output visualization
print("feature_1_5_mean:")
print(df['feature_1_5_mean'].head())

"""!!!! so here we need to add also feature 2, n=because it so much corelated"""

# 2. Ratio Feature
df['feature_2_3_ratio'] = df['feature_2'] / (df['feature_3'] + 1e-5)

# Output visualization
print("feature_2_3_ratio")
print(df['feature_2_3_ratio'].head())

# 3. Iteraction Term
df['feature_2_4_interaction'] = df['feature_2'] * df['feature_4']

# Output visualization
print("feature_2_4_interaction:")
print(df[['feature_2_4_interaction']].head())

# 4. Compute total service usage as the sum of all numerical features
df["total_service_usage"] = df["feature_1"] + df["feature_2"]+ df["feature_3"] + df["feature_4"] + df["feature_5"] + df["feature_6"] + df["feature_7"] + df["feature_8"]

# Verify the new feature
print("After adding 'total_service_usage':")
print(df[["total_service_usage"]].head())

print(df["total_service_usage"].describe())

print(df.columns)

# List of your engineered features
engineered_features = [
    'feature_1_5_mean',
    'feature_2_3_ratio',
    'feature_2_4_interaction',
    'total_service_usage'
]

# Check for missing values
missing_values = df[engineered_features].isnull().sum()

# Combine both into a readable summary
missing_summary = pd.DataFrame({
    'Missing Count': missing_values,
})

print("Missing Values in Engineered Features:\n")
print(missing_summary)

engineered = ['feature_1_5_mean', 'feature_2_3_ratio', 'feature_2_4_interaction', 'total_service_usage']
df[engineered] = scaler.fit_transform(df[engineered])

# # Save cleaned dataset to CSV
# df.to_csv('newFeatures_cleaned_final_dataset.csv', index=False)

# # To download:
# from google.colab import files
# files.download('newFeatures_cleaned_final_dataset.csv')

# Select only numeric columns
numeric_df = df.select_dtypes(include=['number'])

# Compute the correlation matrix
corr_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(16, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.show()

"""# 5. Modeling Activity: Build and Evaluate Models  

*   Description: Train different machine learning models. No less than 3 ensemble learning models must be trained.  
*   Deliverables: D5.1) Section 5 of the final report comparing modelperformances;
D5.2) Trained models.
*   Guidelines:  
*   Split the data into training and testing sets.  
*   Train several models (e.g., logistic regression, decision trees, random forest, gradient boosting, ensemble learning models).  
*   Evaluate models using metrics like accuracy, precision, recall, F1-score, and ROC-AUC.  
*   Perform cross-validation to ensure robust performance estimates.
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

X = df.drop(columns=['target'])
y = df['target']

# Split into training and testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, learning_rate=0.5, random_state=42)
}

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve
)

def train_evaluate_plot(model, X_train, y_train, X_test, y_test, model_name='Model', plot_roc=True, plot_confusion=True):
      """
    Train the model, evaluate metrics, and plot confusion matrix and ROC curve.

    Args:
        model: scikit-learn estimator
        X_train, y_train: training data
        X_test, y_test: testing data
        model_name: str, label for plots
        plot_roc: bool, whether to plot ROC curve
    Returns:
        dict of metrics
    """

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None

    # Metrics calculation
    metrics = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred),
        'Confusion Matrix': confusion_matrix(y_test, y_pred),
        # 'Classification Report': classification_report(y_test, y_pred, output_dict=True)
    }
    if y_proba is not None:
        try:
            metrics['ROC-AUC'] = roc_auc_score(y_test, y_proba)
        except:
            metrics['ROC-AUC'] = None
    else:
        metrics['ROC-AUC'] = None

    # Plot confusion matrix
    if plot_confusion:
        plt.figure(figsize=(6, 4))
        sns.heatmap(metrics['Confusion Matrix'], annot=True, fmt='d', cmap='Blues')
        plt.title(f'Confusion Matrix: {model_name}')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.tight_layout()
        plt.show()

    # Print metrics
    print(f"\n{model_name} - Evaluation Summary:\n")
    for key, value in metrics.items():
        if key != 'Confusion Matrix':
            if isinstance(value, dict):
                print(f"\n{key}:\n")
                print(classification_report(y_test, y_pred))
            elif value is not None:
                print(f"{key:<12}: {value:.4f}")
            else:
                print(f"{key:<12}: N/A")

    return metrics

results_dict = {}
for name, model in models.items():
    print(f"\nTraining and evaluating {name} with Engineered Features")
    metrics = train_evaluate_plot(model, X_train, y_train, X_test, y_test, model_name=name)
    results_dict[name] = metrics

def create_results_df(results):
    """
    Convert results dictionary to a sorted DataFrame.
    """
    df = pd.DataFrame(results).T
    df = df.round(4)
    # Sorting based on multiple columns
    sort_cols = ['Accuracy', 'F1-score', 'ROC-AUC', 'Recall']
    df = df.sort_values(by=sort_cols, ascending=False).reset_index().rename(columns={'index': 'Model'})
    df.insert(0, 'Rank', range(1, len(df) + 1))
    return df

from IPython.display import display
results_df = create_results_df(results_dict)
print("\nModel Evaluation With Engineered Features:\n")
display(results_df.style.hide(axis="index"))

X_baseline = X.drop(columns=engineered_features, errors='ignore')

Xb_train, Xb_test, yb_train, yb_test = train_test_split(
    X_baseline, y, test_size=0.2, random_state=42
)

baseline_results = {}
for name, model in models.items():
    print(f"\nTraining and evaluating {name} without Engineered Features")
    baseline = train_evaluate_plot(model, Xb_train, yb_train, Xb_test, yb_test, model_name=name)
    baseline_results[name] = baseline

bresults_df = create_results_df(baseline_results)
print("\nModel Evaluation Without Engineered Features:\n")
display(bresults_df.style.hide(axis="index"))

from sklearn.model_selection import cross_val_score

def cross_validate_models(models, X, y, cv_splits=5):
    """
    Perform cross-validation for multiple models and return a DataFrame.
    """
    results = {}
    for name, model in models.items():
        scores = {}
        scores['Accuracy (CV avg)'] = cross_val_score(model, X, y, scoring='accuracy', cv=cv_splits, n_jobs=-1).mean()
        scores['Precision (CV avg)'] = cross_val_score(model, X, y, scoring='precision', cv=cv_splits, n_jobs=-1).mean()
        scores['Recall (CV avg)'] = cross_val_score(model, X, y, scoring='recall', cv=cv_splits, n_jobs=-1).mean()
        scores['F1-score (CV avg)'] = cross_val_score(model, X, y, scoring='f1', cv=cv_splits, n_jobs=-1).mean()
        scores['ROC-AUC (CV avg)'] = cross_val_score(model, X, y, scoring='roc_auc', cv=cv_splits, n_jobs=-1).mean()
        results[name] = scores
    df = pd.DataFrame(results).T
    df = df.round(4)
    df = df.sort_values(by=['Accuracy (CV avg)', 'F1-score (CV avg)', 'ROC-AUC (CV avg)', 'Recall (CV avg)'], ascending=False)
    df.insert(0, 'Rank', range(1, len(df) + 1))
    return df

cv_results_df = cross_validate_models(models, X, y, cv_splits=5)
display(cv_results_df.style.set_caption("Cross-Validation Results for Ensemble Models"))

display(results_df.style.set_caption("Model Evaluation With Engineered Features").hide(axis="index"))
display(bresults_df.style.set_caption("Model Evaluation Without Engineered Features").hide(axis="index"))

"""# 6. Model Tuning Activity: Hyperparameter Tuning

*   Description: Optimize model hyperparameters to improve performance.  
*   Deliverables: D6.1) Section 6 of the final report (titled “Hyperparameter tuning”) on the tuning process and results;  
D6.2) Best-tuned models.  

Guidelines:  

*   Use techniques like grid search or randomized search for hyperparameter tuning.  
*   Compare the performance of tuned models against default models.  
*   Document the tuning process and the chosen hyperparameters.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def tune_and_evaluate(model_name, model, param_grid, X_train, y_train, X_test, y_test, results_dict):
    print(f"Tuning {model_name}...")
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring='accuracy',
        cv=5,
        n_jobs=-1
    )
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_

    y_pred = best_model.predict(X_test)
    y_prob = best_model.predict_proba(X_test)[:, 1]

    results_dict[model_name] = {
        'Best Params': str(grid_search.best_params_),
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-score': f1_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_prob)
    }
    print(f"{model_name} tuning complete.\n")

tuned_results = {}

# Random Forest
tune_and_evaluate(
    'Random Forest',
    RandomForestClassifier(random_state=42),
    {
        'n_estimators': [15, 25, 50],
        'max_depth': [10, 12],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [2, 3]
    },
    X_train, y_train, X_test, y_test,
    tuned_results
)

# Gradient Boosting
tune_and_evaluate(
    'Gradient Boosting',
    GradientBoostingClassifier(random_state=42),
    {
        'n_estimators': [50, 100],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 4]
    },
    X_train, y_train, X_test, y_test,
    tuned_results
)

# AdaBoost
tune_and_evaluate(
    'AdaBoost',
    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), random_state=42),
    {
        'n_estimators': [50, 100],
        'learning_rate': [0.5, 1.0]
    },
    X_train, y_train, X_test, y_test,
    tuned_results
)

pd.set_option('display.max_colwidth', None)  # Dro prevent column truncating

tresults_df = pd.DataFrame.from_dict(tuned_results, orient='index')
display(tresults_df)

"""# 7. Model Interpretation Activity: Interpret the Model

*   Description: Analyze the models to understand which features are most important for predictions.
*   Deliverables: D7.1) Section 7 of the final report (titled “Model interpretation”) on feature importance and model interpretation.  
*   Guidelines:  
*   Use feature importance scores from models like decision trees or random forests.  
*   Apply SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable
Model-agnostic Explanations) for detailed model interpretation.  
*   Discuss how the insights align with domain knowledge and business logic.
"""

import shap
for name, model in models.items():
    # Fit model
    model.fit(X, y)

    # Extract feature importances
    if hasattr(model, 'feature_importances_'):
        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=True)
    else:
        print(f"{name} does not have feature_importances_ attribute.")
        continue

    # Plot top 10 features
    plt.figure(figsize=(10, 6))
    sns.barplot(x=importances.values[-10:], y=importances.index[-10:], hue=importances.index[-10:], palette="viridis", legend=False)
    plt.title(f"Top 10 Features - {name}")
    plt.xlabel("Importance Score")
    plt.ylabel("Features")
    plt.tight_layout()
    plt.show()

import lime
import lime.lime_tabular
from lime.lime_tabular import LimeTabularExplainer
import warnings

# Suppress specific scikit-learn UserWarnings
warnings.filterwarnings("ignore", category=UserWarning, module='sklearn.utils.validation')

# Initialize the explainer with training data as a numpy array
explainer = LimeTabularExplainer(
    training_data=X.values,  # numpy array
    feature_names=X.columns.tolist(),
    class_names=['Class0', 'Class1'],  # Adjust as needed
    mode='classification'
)

# Pick a sample index
i = 0
sample = X.iloc[i]

for name, model in models.items():
    # Predict probability
    pred_proba = model.predict_proba(sample.values.reshape(1, -1))[0]
    print(f"\nExplaining {name} prediction")
    explanation = explainer.explain_instance(
        data_row=sample.values,  # Pass as numpy array
        predict_fn=model.predict_proba,
        num_features=10
    )

    explanation.show_in_notebook(show_table=True)